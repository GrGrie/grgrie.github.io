<!---
layout: post
title: "Template"
date: 2025-01-21
author_profile: true
tags: [template, example]
categories: [Personal]
---

**Table of contents**
- [Introduction](#introduction)
- [Text here](#test-here)
  - [Subsection](#subsection)
- [Ending](#ending)



## Introduction
123123

## Text here
### Subsection


$$
\begin{equation}
    \text{OT}(\mu, \nu) = \min_{\pi \in \Pi(\mu, \nu)} \langle C,\pi \rangle
\end{equation}
$$

**Training deep networks with BoMb-OT loss:** In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them. We propose a three-step algorithm to train neural networks with BoMb-OT loss as follows.

![Text of image](/assets/img/link_to_image.png){:style="display:block; margin-left:auto; margin-right:auto"}

## Ending
That's all for now.