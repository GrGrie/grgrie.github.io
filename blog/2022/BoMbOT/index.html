<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img 
  src="/assets/img/ot_example.png"
  class="img-fluid rounded z-depth-1"  
  width="auto" 
  height="auto" 
   
   
   
   
   
   
  data-zoomable
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. One source of the issue is that all pairs of mini-batches are treated the same.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mot_example2-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mot_example2-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mot_example2-1400.webp"></source> <img src="/assets/img/mot_example2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="batch-of-mini-batches-optimal-transport">Batch of Mini-batches Optimal Transport</h2> <p>To address the issues of m-OT, we solve an additional OT problem between mini-batches to find an optimal weighting for combining local mini-batch losses.</p> \[\begin{equation} \text{BoMb-OT}^m(\mu, \nu) = \inf_{\gamma \in \Gamma(\overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu})} \mathbb{E}_{(X, Y) \sim \gamma} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical BoMb-OT reads:</p> \[\begin{equation} \text{BoMb-OT}_k^m(\mu, \nu) \approx \inf_{\gamma \in \Gamma(\overset{\otimes m}{\mu_k} \otimes \overset{\otimes m}{\nu_k})} \sum_{i=1}^k \sum_{j=1}^k \gamma_{ij}[\text{OT}(P_{X_i}, P_{Y_j})] \end{equation}\] <p>where \(X_1, \ldots, X_k \sim \overset{\otimes m}{\mu}\) and \(\overset{\otimes m}{\mu_k} = \frac{1}{k} \sum_{i=1}^k \delta_{X_i}\). \(Y_j (1 \leq j \leq k)\) and \(\overset{\otimes m}{\nu_k}\) are defined similarly.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bombot_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bombot_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bombot_example-1400.webp"></source> <img src="/assets/img/bombot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of BoMb-OT with \(n = 4, m = 2\) and \(k = 2\). After solving the OT problem between mini-batches, \(X_1\) is mapped to \(Y_2\) and \(X_2\) is mapped to \(Y_1\), which results in the same solution as the full-scale optimal transport.</p> <p><strong>Training deep networks with BoMb-OT loss:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them. We propose a three-step algorithm to train neural networks with BoMb-OT loss as follows.</p> <p><img src="/assets/img/training_bombot_loss.png" alt="training_bombot_loss" style="display:block; margin-left:auto; margin-right:auto"></p> <h2 id="experiments">Experiments</h2> <p>BoMb-(U)OT shows a favorable performance compared to m-(U)OT on three types of applications, namely, <em>gradient-based</em> (e.g., deep generative model, deep domain adaptation (DA)), <em>mapping-based</em> (e.g., color transfer), and <em>value-based</em> (e.g., approximate Bayesian computation (ABC)).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/generative_model.png" alt="generative_model" style="display:block; margin-left:auto; margin-right:auto"> Table 1. Comparison between the BoMb-OT and the m-OT on deep generative models. On the MNIST dataset, we evaluate the performances of generators by computing approximated Wasserstein-2 while we use the FID score on CIFAR10 and CelebA.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/BoMbOT_DA_VisDA.png" alt="BoMbOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"> Table 2. Comparison between two mini-batch schemes on the deep domain adaptation on the VisDA dataset. We varied the number of mini-batches k and reported the classification accuracy on the target domain.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/color_transfer-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/color_transfer-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/color_transfer-1400.webp"></source> <img src="/assets/img/color_transfer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Experimental results on color transfer for full OT, the m-OT, and the BoMb-OT on natural images with \((k; m) = (10; 10)\). Color palettes are shown under corresponding images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ABC-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ABC-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ABC-1400.webp"></source> <img src="/assets/img/ABC.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 5. Approximated posteriors from ABC with the m-OT and the BoMb-OT. The first row, the second row, and the last row have \(m = 8, m = 16\), and \(m = 32\), respectively. In each row, the number of mini-batches k is 2; 4; 6; and 8 from left to right.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have presented a novel mini-batch method for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT). The idea of the BoMb-OT is to consider the optimal transport problem on the space of mini-batches with an OT-types ground metric. More importantly, we have shown that the BoMb-OT can be implemented efficiently and they have more favorable performance than the m-OT in various applications of optimal transport including deep generative models, deep domain adaptation, color transfer, approximate Bayesian computation, and gradient flow. For future work, we could consider a hierarchical approach version of optimal transport between incomparable spaces. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf" rel="external nofollow noopener" target="_blank">https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p> </body></html>