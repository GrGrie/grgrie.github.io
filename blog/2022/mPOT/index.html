<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img 
  src="/assets/img/ot_example.png"
  class="img-fluid rounded z-depth-1"  
  width="auto" 
  height="auto" 
   
   
   
   
   
   
  data-zoomable
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \times \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Misspecified matchings issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. We call these pairs misspecified matchings since they are optimal on the local mini-batch scale but they are non-optimal on the global scale. The reason is that all samples in mini-batches are forced to be transported.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mot_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mot_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mot_example-1400.webp"></source> <img src="/assets/img/mot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="mini-batch-partial-optimal-transport">Mini-batch Partial Optimal Transport</h2> <p>To alleviate misspecified matchings, we use partial optimal transport between mini-batches levels instead of optimal transport. The partial optimal transport is defined almost the same as optimal transport except it only allows a fraction of masses $s$ to be transported.</p> <h3 id="partial-optimal-transport">Partial Optimal Transport</h3> <p>Let \(\mu, \nu\) be discrete distributions of \(n\) supports. Given the fraction of masses \(0 \leq s \leq 1\), the Partial Optimal Transport (POT) problem reads:</p> \[\begin{equation} \text{POT}^s(\mu, \nu) = \min_{\pi \in \Pi_s(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi_s(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 \leq \mu, \pi^T 1 \leq \nu, 1^T \pi 1 = s \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <h3 id="mini-batch-partial-optimal-transport-1">Mini-batch Partial Optimal Transport</h3> <p>Similar to m-OT, we define mini-batch POT (m-POT) which averages the partial optimal transport between mini-batches of size \(m \geq 1\) as:</p> \[\begin{equation} \text{m-POT}^{m,s}(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{POT}^s(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-POT reads:</p> \[\begin{equation} \text{m-POT}_k^{m, s}(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{POT}^s(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mpot_example-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mpot_example-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mpot_example-1400.webp"></source> <img src="/assets/img/mpot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of m-POT with \(n = 4, m = 2, k = 2\) and \(s = \frac{1}{2}\).</p> <p>In Figure 3, POT gives the exact 2 matchings, alleviating the misspecified matchings issue.</p> <h3 id="training-deep-networks-with-m-pot-loss">Training deep networks with m-POT loss</h3> <p><strong>Parallel training:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them.</p> <p><strong>Two-stage training:</strong> We also propose two-stage training for <em>domain adaptation</em>. We first find matchings between pairs of bigger mini-batches of size \(M\) on RAM. Then use it to obtain a mapping to create smaller mini-batches of size $m$ which are used on GPU for estimating the gradient of neural networks. This algorithm allows us to have better matchings since they are obtained from larger transportation problems.</p> <h2 id="experiments">Experiments</h2> <p>To validate the performance of the proposed methods, we carry out experiments on deep domain adaptation (DA). We observe that m-POT gives better-adapted classification accuracy on all datasets than the previous methods. Moreover, the two-stage training significantly improves the performance of DA on Office-Home and VisDA for both optimal transport and partial optimal transport.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_digits.png" alt="mPOT_DA_digits" style="display:block; margin-left:auto; margin-right:auto"> Table 1. DA results in classification accuracy on digits datasets (higher is better).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mPOT_DA_OfficeHome-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mPOT_DA_OfficeHome-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mPOT_DA_OfficeHome-1400.webp"></source> <img src="/assets/img/mPOT_DA_OfficeHome.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Table 2. DA results in classification accuracy on the Office-Home dataset (higher is better).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_VisDA.png" alt="mPOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"> Table 3. DA results in classification accuracy on the VisDA dataset (higher is better).</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have introduced a novel mini-batch approach that is referred to as mini-batch partial optimal transport (m-POT). The new mini-batch approach is motivated by the issue of misspecified mappings in the conventional mini-batch optimal transport approach (m-OT). Via extensive experiment studies, we demonstrate that m-POT can perform better than current mini-batch methods including m-OT and m-UOT in domain adaptation applications. Furthermore, we propose the two-stage training approach for the deep DA that outperforms the conventional implementation. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf" rel="external nofollow noopener" target="_blank">https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p> </body></html>