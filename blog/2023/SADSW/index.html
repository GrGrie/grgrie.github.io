<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img 
  src="/assets/img/pc_reconstruction.PNG"
  class="img-fluid rounded z-depth-1"  
  width="auto" 
  height="auto" 
   
   
   
   
   
   
  data-zoomable
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p style="text-align:center; font-style: italic">Figure 1. Overview of Point-cloud Reconstruction.</p> <h3 id="max-sliced-wasserstein-distance">Max Sliced Wasserstein Distance</h3> <p>Max sliced Wasserstein (Max-SW) distance between \(\mu \in \mathcal{P}_p(\mathbb{R}^d)\) and \(\nu\in \mathcal{P}_p(\mathbb{R}^d)\) is:</p> \[\begin{equation} \text{Max-SW}_p(\mu,\nu) = \max_{\theta \in \mathbb{S}^{d - 1}} W_p(\theta\sharp \mu,\theta \sharp \nu), \end{equation}\] <p>where the Wasserstein distance has a closed form on one dimension which is</p> \[\begin{equation} W_p(\mu,\nu) = \left( \int_0^1 |F_\mu^{-1}(z) - F_{\nu}^{-1}(z)|^{p} dz \right)^{1/p}, \end{equation}\] <p>with \(F^{-1}_{\mu}\) and \(F^{-1}_{\nu}\) are the inverse CDF of \(\mu\) and \(\nu\) respectively.</p> <p><strong>Max sliced point-cloud reconstruction:</strong> Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma }\mathbb{E} \left[\max_{\theta \in \mathbb{S}^{d-1}}W_p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\right], \end{equation}\] <p>the one-dimensional Wasserstein between two projected point-clouds can be solved with the time complexity \(\mathcal{O}(m\log m)\).</p> <h3 id="amortized-projection-optimization">Amortized Projection Optimization</h3> <p>Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)}[W_p(\theta_{\psi,\gamma,\phi}\sharp P_X,\theta_{\psi,\gamma,\phi} \sharp P_{g_\gamma (f_\phi(X))})], \end{equation}\] <p>where \(\theta_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\).</p> <h2 id="self-attention-amortized-distributional-projection-optimization">Self-Attention Amortized Distributional Projection Optimization</h2> <h3 id="amortized-distributional-projection-optimization">Amortized Distributional Projection Optimization</h3> <p>Amortized optimization often leads to sub-optimality. Hence, it loses the metricity property since the Max-SW only obtains the identity of indiscernibles at the global optimum. Therefore, we propose to predict an entire distribution over projecting directions.</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon_{\psi,\gamma,\phi},\kappa)} W_p^p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\Big)^{\frac{1}{p}}, \end{equation}\] <p>where \(\epsilon_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\), \(\text{vMF}(\epsilon,\kappa)\) is the von Mises Fisher distribution with the mean location parameter \(\epsilon \in \mathbb{S}^{d-1}\) and the concentration parameter \(\kappa &gt; 0\), and</p> \[\begin{equation} \text{v-DSW}_p(\mu,\nu;\kappa) =\max_{\epsilon \in \mathbb{S}^{d-1}} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon,\kappa)} \text{W}_p^p(\theta \sharp \mu,\theta \sharp \nu) \Big)^{\frac{1}{p}} \end{equation}\] <p>is the von Mises-Fisher distributional sliced Wasserstein distance.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/amsw_avsw.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/amsw_avsw.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/amsw_avsw.PNG-1400.webp"></source> <img src="/assets/img/amsw_avsw.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. The difference between amortized projection optimization and amortized distributional projection optimization.</p> <h3 id="self-attention-amortized-models">Self-Attention Amortized Models</h3> <p>Based on the self-attention mechanism, we introduce the self-attention amortized model which is permutation invariant and symmetric. Given \(X,Y \in \mathbb{R}^{dm}\), the <em>self-attention amortized model</em> is defined as:</p> \[\begin{equation} a_\psi (X,Y)=\frac{\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}}{||\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}||_2}, \end{equation}\] <p>where \(X'\) and \(Y'\) are matrices of size \(d\times m\) that are reshaped from the concatenated vectors \(X\) and \(Y\) of size \(dm\), \(\boldsymbol{1}_{m}\) is the $m$-dimensional vector whose all entries are \(1\), and \(\mathcal{A}_{\zeta}(\cdot)\) is linear (efficient) attention module [5, 6] for preserving near-linear complexity.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/amortized_models.PNG-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/amortized_models.PNG-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/amortized_models.PNG-1400.webp"></source> <img src="/assets/img/amortized_models.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. Visualization of an amortized model that is not symmetric and permutation invariant in two dimensions.</p> <h2 id="experiments">Experiments</h2> <p>To verify the effectiveness of our proposal, we evaluate our methods on the point-cloud reconstruction task and its two downstream tasks including transfer learning and point-cloud generation (please see our papers for more details).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/reconstruction_quantitative.PNG" alt="reconstruction_quantitative" style="display:block; margin-left:auto; margin-right:auto"> Table 1. Reconstruction and transfer learning performance on the ModelNet40 dataset. CD and SW are multiplied by 100.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reconstruction_qualitative-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reconstruction_qualitative-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reconstruction_qualitative-1400.webp"></source> <img src="/assets/img/reconstruction_qualitative.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Qualitative results of reconstructing point-clouds in the ShapeNet Core-55 dataset. From top to bottom, the point-clouds are input, SW, Max-SW, v-DSW, and $\mathcal{L}\mathcal{A}$v-DSW respectively.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have proposed a self-attention amortized distributional projection optimization framework which uses a self-attention amortized model to predict the best discriminative distribution over projecting direction for each pair of probability measures. The efficient self-attention mechanism helps to inject the geometric inductive biases which are permutation invariance and symmetry into the amortized model while remaining fast computation. Furthermore, the amortized distribution projection optimization framework guarantees the metricity for all pairs of probability measures while the amortization gap still exists. On the experimental side, we compare the new proposed framework to the conventional amortized projection optimization framework and other widely-used distances in the point-cloud reconstruction application and its two downstream tasks including transfer learning and point-cloud generation to show the superior performance of the proposed framework. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf" rel="external nofollow noopener" target="_blank">https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Nguyen, T., Pham, Q.-H., Le, T., Pham, T., Ho, N., and Hua,B.-S. Point-set distances for learning representations of 3d point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p> <p>[2] Naderializadeh, N., Comer, J., Andrews, R., Hoffmann, H., and Kolouri, S. Pooling by sliced-Wasserstein embedding. Advances in Neural Information Processing Systems, 34, 2021.</p> <p>[3] Deshpande, I., Hu, Y.-T., Sun, R., Pyrros, A., Siddiqui, N., Koyejo, S., Zhao, Z., Forsyth, D., and Schwing, A. G. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10648–10656, 2019.</p> <p>[4] Nguyen, K. and Ho, N. Amortized projection optimization for sliced Wasserstein generative models. Advances in Neural Information Processing Systems, 2022.</p> <p>[5] Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531–3539, 2021.</p> <p>[6] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.</p> </body></html>