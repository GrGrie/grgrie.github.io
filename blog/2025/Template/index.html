<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.0 Transitional//EN" "http://www.w3.org/TR/REC-html40/loose.dtd"> <html><body> <p>&lt;!— layout: post title: “Template” date: 2025-01-21 author_profile: true tags: [template, example] categories: [Personal] —</p> <p><strong>Table of contents</strong></p> <ul> <li><a href="#introduction">Introduction</a></li> <li> <a href="#test-here">Text here</a> <ul> <li><a href="#subsection">Subsection</a></li> </ul> </li> <li><a href="#ending">Ending</a></li> </ul> <h2 id="introduction">Introduction</h2> <p>123123</p> <h2 id="text-here">Text here</h2> <h3 id="subsection">Subsection</h3> \[\begin{equation} \text{OT}(\mu, \nu) = \min_{\pi \in \Pi(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p><strong>Training deep networks with BoMb-OT loss:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them. We propose a three-step algorithm to train neural networks with BoMb-OT loss as follows.</p> <p><img src="/assets/img/link_to_image.png" alt="Text of image" style="display:block; margin-left:auto; margin-right:auto"></p> <h2 id="ending">Ending</h2> <p>That’s all for now.</p> </body></html>