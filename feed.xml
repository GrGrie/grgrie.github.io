<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://grgrie.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://grgrie.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-11-14T17:21:27+00:00</updated><id>https://grgrie.github.io/feed.xml</id><title type="html">blank</title><subtitle>Grigory Grechkin&apos;s homepage </subtitle><entry><title type="html">Sadsw</title><link href="https://grgrie.github.io/blog/2023/SADSW/" rel="alternate" type="text/html" title="Sadsw"/><published>2023-09-12T00:00:00+00:00</published><updated>2023-09-12T00:00:00+00:00</updated><id>https://grgrie.github.io/blog/2023/SADSW</id><content type="html" xml:base="https://grgrie.github.io/blog/2023/SADSW/"><![CDATA[ <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img 
  src="/assets/img/pc_reconstruction.PNG"
  class="img-fluid rounded z-depth-1"  
  width="auto" 
  height="auto" 
   
   
   
   
   
   
  data-zoomable
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p style="text-align:center; font-style: italic">Figure 1. Overview of Point-cloud Reconstruction.</p> <h3 id="max-sliced-wasserstein-distance">Max Sliced Wasserstein Distance</h3> <p>Max sliced Wasserstein (Max-SW) distance between \(\mu \in \mathcal{P}_p(\mathbb{R}^d)\) and \(\nu\in \mathcal{P}_p(\mathbb{R}^d)\) is:</p> \[\begin{equation} \text{Max-SW}_p(\mu,\nu) = \max_{\theta \in \mathbb{S}^{d - 1}} W_p(\theta\sharp \mu,\theta \sharp \nu), \end{equation}\] <p>where the Wasserstein distance has a closed form on one dimension which is</p> \[\begin{equation} W_p(\mu,\nu) = \left( \int_0^1 |F_\mu^{-1}(z) - F_{\nu}^{-1}(z)|^{p} dz \right)^{1/p}, \end{equation}\] <p>with \(F^{-1}_{\mu}\) and \(F^{-1}_{\nu}\) are the inverse CDF of \(\mu\) and \(\nu\) respectively.</p> <p><strong>Max sliced point-cloud reconstruction:</strong> Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma }\mathbb{E} \left[\max_{\theta \in \mathbb{S}^{d-1}}W_p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\right], \end{equation}\] <p>the one-dimensional Wasserstein between two projected point-clouds can be solved with the time complexity \(\mathcal{O}(m\log m)\).</p> <h3 id="amortized-projection-optimization">Amortized Projection Optimization</h3> <p>Instead of solving all optimization problems independently, an amortized model is trained to predict optimal solutions to all problems. Given a parametric function \(a_\psi: \mathcal{X}\times \mathcal{X} \to \mathbb{S}^{d-1}\) (\(\psi \in \Psi\)), the amortized objective is:</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)}[W_p(\theta_{\psi,\gamma,\phi}\sharp P_X,\theta_{\psi,\gamma,\phi} \sharp P_{g_\gamma (f_\phi(X))})], \end{equation}\] <p>where \(\theta_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\).</p> <h2 id="self-attention-amortized-distributional-projection-optimization">Self-Attention Amortized Distributional Projection Optimization</h2> <h3 id="amortized-distributional-projection-optimization">Amortized Distributional Projection Optimization</h3> <p>Amortized optimization often leads to sub-optimality. Hence, it loses the metricity property since the Max-SW only obtains the identity of indiscernibles at the global optimum. Therefore, we propose to predict an entire distribution over projecting directions.</p> \[\begin{equation} \min_{\phi \in \Phi,\gamma \in \Gamma}\max_{ \psi \in \Psi}\mathbb{E}_{X \sim p(X)} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon_{\psi,\gamma,\phi},\kappa)} W_p^p(\theta \sharp P_X,\theta \sharp P_{g_\gamma (f_\phi(X))})\Big)^{\frac{1}{p}}, \end{equation}\] <p>where \(\epsilon_{\psi,\gamma,\phi} = a_\psi(X,g_\gamma (f_\phi(X)))\), \(\text{vMF}(\epsilon,\kappa)\) is the von Mises Fisher distribution with the mean location parameter \(\epsilon \in \mathbb{S}^{d-1}\) and the concentration parameter \(\kappa &gt; 0\), and</p> \[\begin{equation} \text{v-DSW}_p(\mu,\nu;\kappa) =\max_{\epsilon \in \mathbb{S}^{d-1}} \Big(\mathbb{E}_{\theta \sim \text{vMF}(\epsilon,\kappa)} \text{W}_p^p(\theta \sharp \mu,\theta \sharp \nu) \Big)^{\frac{1}{p}} \end{equation}\] <p>is the von Mises-Fisher distributional sliced Wasserstein distance.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/amsw_avsw.PNG-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/amsw_avsw.PNG-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/amsw_avsw.PNG-1400.webp"/> <img src="/assets/img/amsw_avsw.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. The difference between amortized projection optimization and amortized distributional projection optimization.</p> <h3 id="self-attention-amortized-models">Self-Attention Amortized Models</h3> <p>Based on the self-attention mechanism, we introduce the self-attention amortized model which is permutation invariant and symmetric. Given \(X,Y \in \mathbb{R}^{dm}\), the <em>self-attention amortized model</em> is defined as:</p> \[\begin{equation} a_\psi (X,Y)=\frac{\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}}{||\mathcal{A}_{\zeta}(X'^\top)^\top \boldsymbol{1}_{m} + \mathcal{A}_{\zeta}(Y'^\top)^\top \boldsymbol{1}_{m}||_2}, \end{equation}\] <p>where \(X'\) and \(Y'\) are matrices of size \(d\times m\) that are reshaped from the concatenated vectors \(X\) and \(Y\) of size \(dm\), \(\boldsymbol{1}_{m}\) is the $m$-dimensional vector whose all entries are \(1\), and \(\mathcal{A}_{\zeta}(\cdot)\) is linear (efficient) attention module [5, 6] for preserving near-linear complexity.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/amortized_models.PNG-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/amortized_models.PNG-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/amortized_models.PNG-1400.webp"/> <img src="/assets/img/amortized_models.PNG" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. Visualization of an amortized model that is not symmetric and permutation invariant in two dimensions.</p> <h2 id="experiments">Experiments</h2> <p>To verify the effectiveness of our proposal, we evaluate our methods on the point-cloud reconstruction task and its two downstream tasks including transfer learning and point-cloud generation (please see our papers for more details).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/reconstruction_quantitative.PNG" alt="reconstruction_quantitative" style="display:block; margin-left:auto; margin-right:auto"/> Table 1. Reconstruction and transfer learning performance on the ModelNet40 dataset. CD and SW are multiplied by 100.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reconstruction_qualitative-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reconstruction_qualitative-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reconstruction_qualitative-1400.webp"/> <img src="/assets/img/reconstruction_qualitative.jpg" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Qualitative results of reconstructing point-clouds in the ShapeNet Core-55 dataset. From top to bottom, the point-clouds are input, SW, Max-SW, v-DSW, and $\mathcal{L}\mathcal{A}$v-DSW respectively.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have proposed a self-attention amortized distributional projection optimization framework which uses a self-attention amortized model to predict the best discriminative distribution over projecting direction for each pair of probability measures. The efficient self-attention mechanism helps to inject the geometric inductive biases which are permutation invariance and symmetry into the amortized model while remaining fast computation. Furthermore, the amortized distribution projection optimization framework guarantees the metricity for all pairs of probability measures while the amortization gap still exists. On the experimental side, we compare the new proposed framework to the conventional amortized projection optimization framework and other widely-used distances in the point-cloud reconstruction application and its two downstream tasks including transfer learning and point-cloud generation to show the superior performance of the proposed framework. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf">https://proceedings.mlr.press/v202/nguyen23e/nguyen23e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Nguyen, T., Pham, Q.-H., Le, T., Pham, T., Ho, N., and Hua,B.-S. Point-set distances for learning representations of 3d point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.</p> <p>[2] Naderializadeh, N., Comer, J., Andrews, R., Hoffmann, H., and Kolouri, S. Pooling by sliced-Wasserstein embedding. Advances in Neural Information Processing Systems, 34, 2021.</p> <p>[3] Deshpande, I., Hu, Y.-T., Sun, R., Pyrros, A., Siddiqui, N., Koyejo, S., Zhao, Z., Forsyth, D., and Schwing, A. G. Max-sliced Wasserstein distance and its use for GANs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 10648–10656, 2019.</p> <p>[4] Nguyen, K. and Ho, N. Amortized projection optimization for sliced Wasserstein generative models. Advances in Neural Information Processing Systems, 2022.</p> <p>[5] Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient attention: Attention with linear complexities. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pp. 3531–3539, 2021.</p> <p>[6] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!— layout: post title: “Self-Attention Amortized Distributional Projection Optimization for Sliced Wasserstein Point-Cloud Reconstruction” date: 2023-09-12 author_profile: true tags: optimal-transport sliced-wasserstein point-cloud self-attention categories: conference —]]></summary></entry><entry><title type="html">Bombot</title><link href="https://grgrie.github.io/blog/2022/BoMbOT/" rel="alternate" type="text/html" title="Bombot"/><published>2022-08-26T00:00:00+00:00</published><updated>2022-08-26T00:00:00+00:00</updated><id>https://grgrie.github.io/blog/2022/BoMbOT</id><content type="html" xml:base="https://grgrie.github.io/blog/2022/BoMbOT/"><![CDATA[ <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img 
  src="/assets/img/ot_example.png"
  class="img-fluid rounded z-depth-1"  
  width="auto" 
  height="auto" 
   
   
   
   
   
   
  data-zoomable
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. One source of the issue is that all pairs of mini-batches are treated the same.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mot_example2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mot_example2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mot_example2-1400.webp"/> <img src="/assets/img/mot_example2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="batch-of-mini-batches-optimal-transport">Batch of Mini-batches Optimal Transport</h2> <p>To address the issues of m-OT, we solve an additional OT problem between mini-batches to find an optimal weighting for combining local mini-batch losses.</p> \[\begin{equation} \text{BoMb-OT}^m(\mu, \nu) = \inf_{\gamma \in \Gamma(\overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu})} \mathbb{E}_{(X, Y) \sim \gamma} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical BoMb-OT reads:</p> \[\begin{equation} \text{BoMb-OT}_k^m(\mu, \nu) \approx \inf_{\gamma \in \Gamma(\overset{\otimes m}{\mu_k} \otimes \overset{\otimes m}{\nu_k})} \sum_{i=1}^k \sum_{j=1}^k \gamma_{ij}[\text{OT}(P_{X_i}, P_{Y_j})] \end{equation}\] <p>where \(X_1, \ldots, X_k \sim \overset{\otimes m}{\mu}\) and \(\overset{\otimes m}{\mu_k} = \frac{1}{k} \sum_{i=1}^k \delta_{X_i}\). \(Y_j (1 \leq j \leq k)\) and \(\overset{\otimes m}{\nu_k}\) are defined similarly.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/bombot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/bombot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/bombot_example-1400.webp"/> <img src="/assets/img/bombot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of BoMb-OT with \(n = 4, m = 2\) and \(k = 2\). After solving the OT problem between mini-batches, \(X_1\) is mapped to \(Y_2\) and \(X_2\) is mapped to \(Y_1\), which results in the same solution as the full-scale optimal transport.</p> <p><strong>Training deep networks with BoMb-OT loss:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them. We propose a three-step algorithm to train neural networks with BoMb-OT loss as follows.</p> <p><img src="/assets/img/training_bombot_loss.png" alt="training_bombot_loss" style="display:block; margin-left:auto; margin-right:auto"/></p> <h2 id="experiments">Experiments</h2> <p>BoMb-(U)OT shows a favorable performance compared to m-(U)OT on three types of applications, namely, <em>gradient-based</em> (e.g., deep generative model, deep domain adaptation (DA)), <em>mapping-based</em> (e.g., color transfer), and <em>value-based</em> (e.g., approximate Bayesian computation (ABC)).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/generative_model.png" alt="generative_model" style="display:block; margin-left:auto; margin-right:auto"/> Table 1. Comparison between the BoMb-OT and the m-OT on deep generative models. On the MNIST dataset, we evaluate the performances of generators by computing approximated Wasserstein-2 while we use the FID score on CIFAR10 and CelebA.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/BoMbOT_DA_VisDA.png" alt="BoMbOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"/> Table 2. Comparison between two mini-batch schemes on the deep domain adaptation on the VisDA dataset. We varied the number of mini-batches k and reported the classification accuracy on the target domain.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/color_transfer-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/color_transfer-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/color_transfer-1400.webp"/> <img src="/assets/img/color_transfer.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 4. Experimental results on color transfer for full OT, the m-OT, and the BoMb-OT on natural images with \((k; m) = (10; 10)\). Color palettes are shown under corresponding images.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ABC-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ABC-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ABC-1400.webp"/> <img src="/assets/img/ABC.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 5. Approximated posteriors from ABC with the m-OT and the BoMb-OT. The first row, the second row, and the last row have \(m = 8, m = 16\), and \(m = 32\), respectively. In each row, the number of mini-batches k is 2; 4; 6; and 8 from left to right.</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have presented a novel mini-batch method for optimal transport, named Batch of Mini-batches Optimal Transport (BoMb-OT). The idea of the BoMb-OT is to consider the optimal transport problem on the space of mini-batches with an OT-types ground metric. More importantly, we have shown that the BoMb-OT can be implemented efficiently and they have more favorable performance than the m-OT in various applications of optimal transport including deep generative models, deep domain adaptation, color transfer, approximate Bayesian computation, and gradient flow. For future work, we could consider a hierarchical approach version of optimal transport between incomparable spaces. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf">https://proceedings.mlr.press/v162/nguyen22d/nguyen22d.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!— layout: post title: “On Transportation of Mini-batches: A Hierarchical Approach” date: 2022-08-26 author_profile: true tags: optimal-transport domain-adaptation generative-models categories: conference —]]></summary></entry><entry><title type="html">Mpot</title><link href="https://grgrie.github.io/blog/2022/mPOT/" rel="alternate" type="text/html" title="Mpot"/><published>2022-08-26T00:00:00+00:00</published><updated>2022-08-26T00:00:00+00:00</updated><id>https://grgrie.github.io/blog/2022/mPOT</id><content type="html" xml:base="https://grgrie.github.io/blog/2022/mPOT/"><![CDATA[ <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>&lt;img 
  src="/assets/img/ot_example.png"
  class="img-fluid rounded z-depth-1"  
  width="auto" 
  height="auto" 
   
   
   
   
   
   
  data-zoomable
  onerror="this.onerror=null; $('.responsive-img-srcset').remove();"
/&gt;
</code></pre></div></div> <p>&lt;/picture&gt;</p> <p>&lt;/figure&gt;</p> <p style="text-align:center; font-style: italic">Figure 1. An example of OT with \(n = 4\).</p> <h3 id="mini-batch-optimal-transport">Mini-batch Optimal Transport</h3> <p>The original \(n\) samples are divided into random mini-batches of size \(m \geq 1\), then an alternative solution to the original OT problem is formed by averaging these smaller OT solutions.</p> \[\begin{equation} \text{m-OT}^m(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \times \overset{\otimes m}{\nu}} [\text{OT}(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-OT reads:</p> \[\begin{equation} \text{m-OT}_k^m(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{OT}(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1 in previous works.</p> <p><strong>Misspecified matchings issue of m-OT:</strong> We can see that the optimal matchings at the mini-batch level in Figure 2 are different from the full-scale optimal transport. We call these pairs misspecified matchings since they are optimal on the local mini-batch scale but they are non-optimal on the global scale. The reason is that all samples in mini-batches are forced to be transported.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mot_example-1400.webp"/> <img src="/assets/img/mot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 2. An example of m-OT with \(n = 4, m = 2\) and \(k = 2\).</p> <h2 id="mini-batch-partial-optimal-transport">Mini-batch Partial Optimal Transport</h2> <p>To alleviate misspecified matchings, we use partial optimal transport between mini-batches levels instead of optimal transport. The partial optimal transport is defined almost the same as optimal transport except it only allows a fraction of masses $s$ to be transported.</p> <h3 id="partial-optimal-transport">Partial Optimal Transport</h3> <p>Let \(\mu, \nu\) be discrete distributions of \(n\) supports. Given the fraction of masses \(0 \leq s \leq 1\), the Partial Optimal Transport (POT) problem reads:</p> \[\begin{equation} \text{POT}^s(\mu, \nu) = \min_{\pi \in \Pi_s(\mu, \nu)} \langle C,\pi \rangle \end{equation}\] <p>where \(\Pi_s(\mu, \nu) = \{ \pi \in \mathbb{R}_{+}^{n \times n} \mid \pi 1 \leq \mu, \pi^T 1 \leq \nu, 1^T \pi 1 = s \}\) is the set of admissible transportation plans between \(\mu\) and \(\nu\).</p> <h3 id="mini-batch-partial-optimal-transport-1">Mini-batch Partial Optimal Transport</h3> <p>Similar to m-OT, we define mini-batch POT (m-POT) which averages the partial optimal transport between mini-batches of size \(m \geq 1\) as:</p> \[\begin{equation} \text{m-POT}^{m,s}(\mu, \nu) = \mathbb{E}_{(X, Y) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}} [\text{POT}^s(P_X, P_Y)] \end{equation}\] <p>where \(\otimes\) denotes product measure, \(X = (x_1, \ldots, x_m)\) is the sampled mini-batch, and \(P_X = \frac{1}{m} \sum_{i=1}^m \delta_{x_i}\) is the corresponding discrete distribution. In practice, we can use subsampling to approximate the expectation, thus the empirical m-POT reads:</p> \[\begin{equation} \text{m-POT}_k^{m, s}(\mu, \nu) \approx \frac{1}{k} \sum_{i=1}^k [\text{POT}^s(P_{X_i}, P_{Y_i})] \end{equation}\] <p>where \((X_1, Y_1), \ldots, (X_k, Y_k) \sim \overset{\otimes m}{\mu} \otimes \overset{\otimes m}{\nu}\) and \(k\) is often set to 1.</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mpot_example-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mpot_example-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mpot_example-1400.webp"/> <img src="/assets/img/mpot_example.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Figure 3. An example of m-POT with \(n = 4, m = 2, k = 2\) and \(s = \frac{1}{2}\).</p> <p>In Figure 3, POT gives the exact 2 matchings, alleviating the misspecified matchings issue.</p> <h3 id="training-deep-networks-with-m-pot-loss">Training deep networks with m-POT loss</h3> <p><strong>Parallel training:</strong> In the deep learning context, the supports are usually parameterized by neural networks. In addition, the gradient of neural networks is accumulated from each pair of mini-batches and only one pair of mini-batches are used in memory at a time. Since the computations on pairs of mini-batches are independent, we can use multiple devices to compute them.</p> <p><strong>Two-stage training:</strong> We also propose two-stage training for <em>domain adaptation</em>. We first find matchings between pairs of bigger mini-batches of size \(M\) on RAM. Then use it to obtain a mapping to create smaller mini-batches of size $m$ which are used on GPU for estimating the gradient of neural networks. This algorithm allows us to have better matchings since they are obtained from larger transportation problems.</p> <h2 id="experiments">Experiments</h2> <p>To validate the performance of the proposed methods, we carry out experiments on deep domain adaptation (DA). We observe that m-POT gives better-adapted classification accuracy on all datasets than the previous methods. Moreover, the two-stage training significantly improves the performance of DA on Office-Home and VisDA for both optimal transport and partial optimal transport.</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_digits.png" alt="mPOT_DA_digits" style="display:block; margin-left:auto; margin-right:auto"/> Table 1. DA results in classification accuracy on digits datasets (higher is better).</p> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mPOT_DA_OfficeHome-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mPOT_DA_OfficeHome-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mPOT_DA_OfficeHome-1400.webp"/> <img src="/assets/img/mPOT_DA_OfficeHome.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p style="text-align:center; font-style: italic">Table 2. DA results in classification accuracy on the Office-Home dataset (higher is better).</p> <p style="text-align:center; font-style: italic"><img src="/assets/img/mPOT_DA_VisDA.png" alt="mPOT_DA_VisDA" style="display:block; margin-left:auto; margin-right:auto"/> Table 3. DA results in classification accuracy on the VisDA dataset (higher is better).</p> <h2 id="conclusion">Conclusion</h2> <p>In this paper, we have introduced a novel mini-batch approach that is referred to as mini-batch partial optimal transport (m-POT). The new mini-batch approach is motivated by the issue of misspecified mappings in the conventional mini-batch optimal transport approach (m-OT). Via extensive experiment studies, we demonstrate that m-POT can perform better than current mini-batch methods including m-OT and m-UOT in domain adaptation applications. Furthermore, we propose the two-stage training approach for the deep DA that outperforms the conventional implementation. For further information, please refer to our work at <a href="https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf">https://proceedings.mlr.press/v162/nguyen22e/nguyen22e.pdf</a>.</p> <h2 id="references">References</h2> <p>[1] Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International Conference on Machine Learning, pp. 214–223, 2017.</p> <p>[2] Courty, N., Flamary, R., Tuia, D., and Rakotomamonjy, A. Optimal transport for domain adaptation. IEEE transactions on pattern analysis and machine intelligence, 39(9):1853–1865, 2016.</p> <p>[3] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch Wasserstein: asymptotic and gradient properties. In AISTATS 2020-23nd International Conference on Artificial Intelligence and Statistics, volume 108, pp. 1–20, 2020.</p> <p>[4] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal transport distances; analysis and applications. arXiv preprint arXiv:2101.01792, 2021b.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[&lt;!— layout: post title: “Improving Mini-batch Optimal Transport via Partial Transportation” date: 2022-08-26 author_profile: true tags: optimal-transport domain-adaptation generative-models categories: conference —]]></summary></entry></feed>